import os
from datetime import datetime
from typing import Any, Dict, Tuple

from ..common.platform_config import get_platform_config
from .engines import docling_engine
from .engines.fallbacks import pdfplumber_fallback
from .postprocess import (
    postprocess_markdown,
    remove_additional_noise,
    autolink_emails_and_urls,
    remove_sections_by_title_regex,
    insert_quick_links,
    add_per_section_link_summaries,
    inline_links_from_section_summaries,
    trim_before_first_main_section,
)


def _normalize_markdown(md: str) -> str:
    import re
    md = re.sub(r"[ \t]+$", "", md, flags=re.MULTILINE)
    md = re.sub(r"\n{3,}", "\n\n", md)
    return md.strip() + "\n"


def run(pdf_path: str, output_path: str) -> Dict[str, Any]:
    cfg = get_platform_config()

    os.makedirs(os.path.dirname(output_path) or ".", exist_ok=True)

    # Primary conversion
    md_text, meta = docling_engine.convert(pdf_path)

    # Fallback tables if requested
    recovered_tables: list[tuple[int, str]] = []
    if (not md_text or meta.get("status") != "ok") and os.getenv("TABLE_FALLBACK_EXTRACTOR", "off") == "pdfplumber":
        recovered_tables = pdfplumber_fallback.recover_tables(pdf_path)

    parts = [
        f"<!-- Generated by Markdown Conversion Pipeline | platform={cfg.platform} | ts={datetime.now().isoformat()} -->",
        md_text if md_text else "> Primary conversion failed or produced empty content.",
    ]
    if recovered_tables:
        parts.append("\n## Recovered Tables (Fallback)\n")
        for page_no, tbl_md in recovered_tables:
            parts.append(f"### Page {page_no}\n\n{tbl_md}\n")

    md = "\n".join(parts)

    # Postprocess
    md = postprocess_markdown(md)
    md = remove_additional_noise(md)
    md = autolink_emails_and_urls(md)

    # Remove Annexes
    if os.getenv("REMOVE_ANNEXES", "true").lower() in {"1", "true", "yes", "on"}:
        md = remove_sections_by_title_regex(md, r"\bannex(es)?\b")

    # Links placement
    if os.getenv("LINK_MODE", "inline_then_fallback") != "off":
        # Extract URLs from PDF annotations to build quick links
        def _extract_pdf_links(path: str) -> list[str]:
            try:
                from pypdf import PdfReader
            except Exception:
                return []
            try:
                reader = PdfReader(path)
                urls: list[str] = []
                for page in reader.pages:
                    try:
                        annots = page.get("/Annots", [])
                        for annot in annots or []:
                            try:
                                obj = annot.get_object()
                                a = obj.get("/A")
                                if a and a.get("/S") == "/URI":
                                    uri = a.get("/URI")
                                    if uri:
                                        urls.append(str(uri))
                            except Exception:
                                continue
                    except Exception:
                        continue
                return urls
            except Exception:
                return []

        urls = _extract_pdf_links(pdf_path)
        md = insert_quick_links(md, urls)
        # Per-section summaries based on URLs found in section body
        md = add_per_section_link_summaries(md, max_links_per_section=12)
        # Attempt inline link wrapping using hostnames from URLs
        md = inline_links_from_section_summaries(md, per_section_limit=3)

    # Trim to first section
    if os.getenv("TRIM_BEFORE_SECTION1", "true").lower() in {"1", "true", "yes", "on"}:
        md = trim_before_first_main_section(md)

    md = _normalize_markdown(md)
    with open(output_path, "w") as f:
        f.write(md)

    return {
        "status": "ok",
        "engine": meta.get("engine", "unknown"),
        "primary_status": meta.get("status", "unknown"),
        "fallback_tables": len(recovered_tables),
        "output_path": output_path,
    }


