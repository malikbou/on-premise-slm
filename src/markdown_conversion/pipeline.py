import os
from datetime import datetime
from typing import Any, Dict, Tuple

from ..common.platform_config import get_platform_config
from .engines import docling_engine
from .engines.fallbacks import pdfplumber_fallback
from .postprocess import (
    postprocess_markdown,
    remove_additional_noise,
    autolink_emails_and_urls,
    remove_sections_by_title_regex,
    insert_quick_links,
    add_per_section_link_summaries,
    inline_links_from_section_summaries,
    trim_before_first_main_section,
    normalize_tables,
    deduplicate_tables,
    clean_symbols,
    reflow_paragraphs,
    deduplicate_headings,
    remove_navigation_cruft_and_bloat,
    fix_malformed_double_links,
    improve_url_display_text,
    create_authentic_inline_links,
    inline_annotations_by_keyword,
)


def _normalize_markdown(md: str) -> str:
    import re
    md = re.sub(r"[ \t]+$", "", md, flags=re.MULTILINE)
    md = re.sub(r"\n{3,}", "\n\n", md)
    return md.strip() + "\n"


def run(pdf_path: str, output_path: str) -> Dict[str, Any]:
    cfg = get_platform_config()

    os.makedirs(os.path.dirname(output_path) or ".", exist_ok=True)

    # Primary conversion
    md_text, meta = docling_engine.convert(pdf_path)

    # Fallback tables if requested
    recovered_tables: list[tuple[int, str]] = []
    if (not md_text or meta.get("status") != "ok") and os.getenv("TABLE_FALLBACK_EXTRACTOR", "off") == "pdfplumber":
        recovered_tables = pdfplumber_fallback.recover_tables(pdf_path)

    parts = [
        f"<!-- Generated by Markdown Conversion Pipeline | platform={cfg.platform} | ts={datetime.now().isoformat()} -->",
        md_text if md_text else "> Primary conversion failed or produced empty content.",
    ]
    if recovered_tables:
        parts.append("\n## Recovered Tables (Fallback)\n")
        for page_no, tbl_md in recovered_tables:
            parts.append(f"### Page {page_no}\n\n{tbl_md}\n")

    md = "\n".join(parts)

    # Postprocess
    md = postprocess_markdown(md)
    md = remove_additional_noise(md)
    md = clean_symbols(md)
    # Link quality cleanup and authentic linking before autolink
    md = fix_malformed_double_links(md)

    def _extract_pdf_links(path: str) -> list[str]:
        try:
            from pypdf import PdfReader
        except Exception:
            return []
        try:
            reader = PdfReader(path)
            urls: list[str] = []
            for page in reader.pages:
                try:
                    annots = page.get("/Annots", [])
                    for annot in annots or []:
                        try:
                            obj = annot.get_object()
                            a = obj.get("/A")
                            if a and a.get("/S") == "/URI":
                                uri = a.get("/URI")
                                if uri:
                                    urls.append(str(uri))
                        except Exception:
                            continue
                except Exception:
                    continue
            return urls
        except Exception:
            return []

    annotations = _extract_pdf_links(pdf_path)
    md = create_authentic_inline_links(md, annotations)
    md = inline_annotations_by_keyword(md, annotations)
    md = improve_url_display_text(md)
    md = autolink_emails_and_urls(md)

    # Remove Annexes
    if os.getenv("REMOVE_ANNEXES", "true").lower() in {"1", "true", "yes", "on"}:
        md = remove_sections_by_title_regex(md, r"\bannex(es)?\b")

    # Links placement: disable adding "Quick Links"/"Links" sections to avoid polluting markdown
    # Keep only inline linking behavior from earlier passes
    # Remove any residual 'Links' sections extracted from source or summaries
    md = remove_sections_by_title_regex(md, r"^#{2,6}\s*Links(\s*\(\d+\))?\s*$")

    # Trim to first section
    if os.getenv("TRIM_BEFORE_SECTION1", "true").lower() in {"1", "true", "yes", "on"}:
        md = trim_before_first_main_section(md)

    # Remove nav-only sections anywhere in document (e.g., Quick Links blocks)
    md = remove_navigation_cruft_and_bloat(md)

    # Additional normalization passes (conservative, no new env flags)
    md = deduplicate_headings(md)
    md = reflow_paragraphs(md)
    md = normalize_tables(md)
    md = deduplicate_tables(md)

    md = _normalize_markdown(md)
    with open(output_path, "w") as f:
        f.write(md)

    return {
        "status": "ok",
        "engine": meta.get("engine", "unknown"),
        "primary_status": meta.get("status", "unknown"),
        "fallback_tables": len(recovered_tables),
        "output_path": output_path,
    }


