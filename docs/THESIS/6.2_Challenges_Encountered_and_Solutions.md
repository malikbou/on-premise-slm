## 6.2 Challenges Encountered and Solutions

Building and evaluating an on‑premise RAG stack surfaced issues that were less about model choice and more about data quality, infrastructure constraints, and operational hygiene. This section documents what failed in practice, how each issue was addressed, and what changed in outcomes, with references to code, runs, and figures.

### 6.2.1 Unstructured PDF Data Extraction

- Symptoms and impact
  - The UCL CS Handbook PDF combined multi‑column layouts, repeated headers/footers, inconsistent numbering, and embedded tables. Early conversions produced broken headings, duplicated navigation blocks, and malformed tables. Retrieval pulled noisy or partial chunks, and context precision plateaued (top configurations generally ≈0.53–0.59; none >0.6), limiting overall RAG quality (see `docs/THESIS/5.1_RAG_Quality_Analysis.md`, Figure 5.5 and discussion around L56–L62).
- Attempts and why they failed
  - Tried generic PDF→Markdown tools that ignored PDF annotations and table structure, leaving link text and tables unusable for chunking. Post‑hoc regex fixes helped little; noise remained in indexes.
- Final solution and effect
  - Implemented a Docling‑first pipeline with targeted post‑processing in `src/markdown_conversion/`:
    - Conversion via `src/markdown_conversion/engines/docling_engine.py` and orchestration in `src/markdown_conversion/pipeline.py` (L42–L67, L115–L123), with optional `pdfplumber` table recovery.
    - Post‑processing passes removed TOC/headers, normalised headings, authentic links from PDF annotations, and table normalisation/deduplication (see `src/markdown_conversion/postprocess.py` and `src/markdown_conversion/README.md`).
  - Outcome: cleaner sections and tables reduced irrelevant chunks retrieved; while absolute precision remained <0.6 across models, the improved structure contributed to the strongest pairs achieving the upper bound of the observed precision range and lifted answer relevancy for top local pairs (5.1, L50–L61).
- Trade‑offs and remaining limitations
  - Docling availability and occasional empty outputs required a conservative fallback. Some complex tables still needed manual checks. Gains are bounded by embedding model limits and long‑context chunking strategy.
- Lesson
  - Prioritise domain‑aware conversion and aggressive post‑processing before index build; invest in authentic links and table hygiene to raise retrieval precision.

### 6.2.2 High‑Quality Testset Generation

- Symptoms and impact
  - Ragas testset generation was brittle: multi‑hop failed with tuple and cluster errors, styles were inconsistent (“can u…”) and combinations were slow, leading to ambiguous or trivial queries that distorted evaluation.
- Attempts and why they failed
  - Following online docs led to mismatched property names and schemas; synthesiser defaults did not enforce style/length and explored too many clusters.
- Final solution and effect
  - Built a controlled workflow in `src/testset/generate_testset.ipynb`, aligning strictly with local Ragas source conventions (see `docs/THESIS/PROBLEMS_SOLUTIONS.md`, 2025‑09‑12 entry):
    - Consistent relation names: `keyphrases`, `keyphrases_overlap`, `keyphrases_overlap_score`.
    - Flattened overlapped items for persona prompts; preserved pairs for combinations.
    - Forced `QueryStyle.PERFECT_GRAMMAR` and fixed `QueryLength` inside synthesiser combinations; capped clusters, set `depth_limit=2`.
    - Pre‑flight checks ensured `generator.knowledge_graph is kg` and non‑zero triplets/clusters.
  - Outcome: coherent, deduplicated questions with controlled style/length and fewer ambiguous items, aligning with Chapter 5.1’s stable rankings and metric profiles.
- Trade‑offs and remaining limitations
  - Notebook‑driven generation remains semi‑manual; abstract queries still need occasional pruning. Multi‑hop breadth is intentionally limited to meet runtime constraints.
- Lesson
  - Treat testset generation as a governed pipeline with explicit naming, schema checks, and capped search breadth; do not rely on prompts alone for quality.

### 6.2.3 Prohibitive Cloud Infrastructure Costs

- Symptoms and impact
  - Iterating with cloud LLMs via a proxy was convenient but financially inefficient for sustained experimentation. Cost per response (CPR) remained orders of magnitude higher than on‑prem under observed throughput.
- Attempts and why they failed
  - Ad‑hoc usage probes (`src/throughput/usage_probe.py`) and token statistics from saved answers (`src/throughput/token_stats_from_answers.py`) confirmed typical/heavy token footprints. Even with careful token budgets, CPR dominated at scale.
- Final solution and effect
  - Shifted sustained runs to a single GPU node (Vast.ai‑style pricing) and computed on‑prem CPR using sustained RPS from throughput results: `results/runs/20250913_223907_vm/throughput/benchmark-results.csv`. Chapter 5.3 shows median‑token CPRs and break‑evens (5.3, table at L48–L53): for example, GPT‑5 ≈ $0.00349 vs on‑prem ≈ $0.00000489 per response, with break‑even around a few thousand requests/day (5.3, L54–L56).
  - Outcome: on‑prem became decisively cheaper beyond modest daily volume, while meeting latency targets (5.2, p95 sub‑second at c≈8 for fast SLMs).
- Trade‑offs and remaining limitations
  - Cloud endpoints still offer elasticity and consistent tails; they remain preferable for spiky workloads or where governance/compliance offload is valued.
- Lesson
  - Quantify CPR with measured tokens and sustained RPS; use on‑prem for steady‑state throughput once break‑even is crossed.

### 6.2.4 Local Inferencing Constraints (Ollama, Mac/Docker)

- Symptoms and impact
  - On an Apple Silicon laptop, Docker lacks direct GPU access; models ran on CPU inside containers or competed for unified memory on host, causing slowdowns and occasional instability when switching models during sweeps.
- Attempts and why they failed
  - Running Ollama inside Docker on Mac starved the models of GPU acceleration; keeping everything in containers simplified networking but degraded performance.
- Final solution and effect
  - Used host‑native Ollama and switched endpoints based on environment:
    - `src/build_index.py` resolves `OLLAMA_BASE_URL` to `http://localhost:11434` on host and `http://host.docker.internal:11434` in containers (L107–L121).
    - Throughput and benchmarking runners allow presets/flags to select host vs container bases and stopping modes (`src/throughput/runner.py` create_parser L325–L351; `resolve_stop_mode` L367–L369; `src/benchmarking/benchmark.py` apply preset defaults L306–L325).
  - Outcome: predictable connectivity without GPU regression on Mac; on VM, `docker-compose.vm.yml` cleanly redirects services to containerised Ollama.
- Trade‑offs and remaining limitations
  - Host/container split adds configuration surface and implicit assumptions; Mac performance remains bounded by shared memory and lack of CUDA.
- Lesson
  - Separate networking from acceleration: call host Ollama from containers on Mac; switch to containerised Ollama on GPU VMs via compose overlays.

### 6.2.5 Memory Management and Model Lifecycle

- Symptoms and impact
  - Loading multiple SLMs back‑to‑back caused RAM/VRAM pressure, sporadic slowdowns, and, in some runs, failure to load the next model cleanly. Long sweeps amplified fragmentation and tail latency.
- Attempts and why they failed
  - Relying on natural eviction or reuse left resident weights in memory; changing only the model name in requests did not always trigger release.
- Final solution and effect
  - Implemented explicit unloads between sweeps:
    - `stop_ollama_model` in `src/benchmarking/benchmark.py` (L355–L375, L476–L481) and `stop_ollama_model_safe` plus `resolve_stop_mode` in `src/throughput/runner.py` (L372–L390, L367–L369, L591–L613) stop models via host or `docker exec` depending on preset.
  - Outcome: reduced memory pressure, steadier RPS at the knee (5.2, L27–L36), and tighter tails for the fastest local models (tail ratio ≈1.05 at c=8–16; 5.2, L64–L70).
- Trade‑offs and remaining limitations
  - Introduces a small overhead between sweeps; does not prevent fragmentation within very long single‑model runs.
- Lesson
  - Treat model weights as lifecycle‑managed resources; stop/unload explicitly between configurations to avoid cumulative pressure.

### 6.2.6 Retrieval Prompt and Chain Robustness (optional)

- Symptoms and impact
  - Early RAG prompts occasionally produced conservative “insufficient context” answers on cloud baselines and overly assertive answers on smaller local models. This affected Answer Relevancy and Faithfulness variance (5.1, L44–L53).
- Final solution and effect
  - Standardised the `/query` contract in `src/main.py` with explicit `model_name` routing and consistent context assembly; maintained a single retrieval chain and prompt scaffold for all providers to keep comparisons fair.
  - Outcome: reduced prompt drift across models and clearer attribution of differences to retrieval/LLM rather than scaffolding.
- Lesson
  - Fix the retrieval pipeline and prompt scaffold when benchmarking; vary only embeddings and generators to maintain comparability.

### Lessons Learned

- Focus early on data conversion quality; it compounds through retrieval and evaluation.
- Govern testset generation with explicit schemas, fixed styles, and pre‑flight checks.
- Use measured tokens and sustained RPS to decide cloud vs on‑prem; avoid guesswork.
- Separate connectivity from acceleration on Mac: host Ollama + Docker DNS.
- Manage model lifecycles explicitly; unload between sweeps to keep tails tight.
- Keep prompts and retrieval chain fixed across experiments for fair comparisons.

### Further Improvements

- Introduce a reranker and/or hybrid BM25+embedding retrieval to raise context precision beyond ~0.6.
- Experiment with adaptive chunking and section‑aware splitting to improve precision/recall balance.
- Add light query rewriting and answer‑length controls in the RAG API for consistency.
- Establish testset governance: deduping heuristics, ambiguity filters, and CI checks for dataset drift.
- Integrate a small validation suite (smoke + regression) for Docling pipeline and index builds.
- Pool GPUs or enable queueing to smooth spikes; explore batched decoding for small models.
- Automate cost dashboards from `results/cost/*` to monitor CPR drift as prompts evolve.
