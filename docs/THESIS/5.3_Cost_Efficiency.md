## 5.3 Cost Efficiency Analysis

This section compares the cost per response (CPR) of cloud APIs against the on‑premise deployment. The cloud CPR derives from provider token pricing and measured token usage; the on‑prem CPR derives from node hourly cost and sustained throughput.

Methodology and assumptions
- **Cloud CPR**: `CPR = (avg_prompt_tokens/1e6)*price_in + (avg_completion_tokens/1e6)*price_out`.
  - Token usage can be collected with the utility: `src/throughput/usage_probe.py` against your LiteLLM gateway (which proxies Anthropic/Google/Azure and returns `usage` fields when providers support it).
  - Alternatively (no new API calls), compute typical token counts from your saved answers using `src/throughput/token_stats_from_answers.py`. Use the median (typical) and p90 (heavy) tokens to produce two CPR numbers for sensitivity.
  - Pricing (USD per 1M tokens, examples):
    - Claude Opus 4.1: input $15, output $75
    - Gemini 2.5 Pro: input $2.5, output $15
    - Azure GPT‑5: input $1.25, output $10 (example; confirm current pricing)
- **On‑prem CPR**: `CPR = hourly_cost / (sustained_RPS * 3600)`.
  - Sustained RPS is taken from the peak stable throughput in `results/runs/<STAMP>_vm/throughput/benchmark-results.csv`.
  - Hourly cost example: $0.356/hr for a single RTX 4090 node (adjust to your actual spend or amortization).
- **Break‑even**: `requests_per_day = (24*hourly_cost) / cloud_CPR`.

Reproducible commands
- Probe cloud usage (example: Azure GPT‑5 via LiteLLM):
  - `python src/throughput/usage_probe.py --base-url http://localhost:4000 --model azure-gpt5 --requests 50 --concurrency 5 --max-tokens 256 --prompt "Summarize the next exam deadlines for CS students." --out results/cost/usage_azure-gpt5.json`
- Compute typical tokens from saved answers (median/p90):
  - `python src/throughput/token_stats_from_answers.py --answers-glob "results/benchmarking/20250913_170609/answers__*.json" --out results/cost/token_stats.json`
- Compute CPRs and break‑even:
  - `python src/throughput/cost_calculator.py --throughput-csv results/runs/20250913_223907_vm/throughput/benchmark-results.csv --hourly-cost 0.356 --avg-prompt-tokens 750 --avg-completion-tokens 150 --price-in-per-mtok 1.25 --price-out-per-mtok 10 --out results/cost/cpr_summary.json`

Illustrative example (replace with your measured usage)
- Cloud (Azure GPT‑5) with avg 750 prompt tokens and 150 completion tokens:
  - `CPR = (750/1e6)*1.25 + (150/1e6)*10 ≈ $0.0009375 + $0.0015 = $0.0024375`
- On‑prem (peak sustained RPS ≈ 21 from Qwen2.5‑3B):
  - `requests_per_hour = 21 * 3600 = 75,600`
  - `CPR = 0.356 / 75,600 ≈ $4.71e-06`
- Break‑even (requests/day):
  - `(24 * 0.356) / 0.0024375 ≈ 3,504`

Interpretation
- For low volumes, cloud pay‑per‑use is convenient and avoids idle cost. As traffic grows toward a few thousand requests/day, on‑prem quickly becomes orders of magnitude cheaper per response—provided a single node can meet latency/throughput SLOs (see 5.2).
- Sensitivity: break‑even scales linearly with hardware cost and inversely with cloud CPR. Falling GPU prices or higher average token usage tilt further in favor of on‑prem; longer completions increase cloud CPR.

Data sources
- Throughput CSV: `../../results/runs/20250913_223907_vm/throughput/benchmark-results.csv`
- Usage (if collected): `../../results/cost/usage_*.json`
 - Token stats (if computed offline): `../../results/cost/token_stats.json`

### Calculated CPR and break‑even (median tokens)

Assumptions: median input 1,525 tokens; median output 158 tokens (from `token_stats.json`). On‑prem hourly cost $0.37; sustained RPS 21.02 (Qwen2.5‑3B at c=8).

| Model | Cloud CPR ($/response) | On‑Prem CPR ($/response) | Break‑even (requests/day) |
| --- | ---:| ---:| ---:|
| GPT‑5 | 0.00349 | 0.00000489 | 2,546 |
| Gemini 2.5 Pro | 0.00618 | 0.00000489 | 1,437 |
| Claude Opus 4.1 | 0.03473 | 0.00000489 | 256 |

Notes
- Daily on‑prem fixed cost = $0.37 × 24 = $8.88. Break‑even = 8.88 ÷ Cloud CPR.
- For readability: cost per 1k responses is ≈ $3.49 (GPT‑5), $6.18 (Gemini), $34.73 (Claude), vs ≈ $0.00489 on‑prem.
- Token counts include question + retrieved context + answer; prompt scaffolding adds a small overhead.
