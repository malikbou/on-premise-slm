## 5.1 RAG Quality Analysis

This section evaluates the effectiveness of the RAG system using the Ragas framework across four metrics: Faithfulness, Answer Relevancy, Context Precision, and Context Recall. Scores are reported per generative model × embedding model pair and summarized via a weighted aggregate to determine the overall ranking.

Context
- All RAG quality runs referenced here were executed on an NVIDIA RTX 4090–class host (same environment described in Chapter 4).
- Source files: `../../results/benchmarking/20250913_170609/summary.json` and `../../results/benchmarking/20250913_170609/figures/overall_ranking.csv`.
- Figures for this section are under `../../results/benchmarking/20250913_170609/figures/`.

Methodology
- Metric definitions (Ragas):
  - **Faithfulness**: factual consistency of the answer against retrieved context (lower hallucination).
  - **Answer Relevancy**: how well the answer addresses the user’s question.
  - **Context Precision**: proportion of retrieved context that is relevant.
  - **Context Recall**: whether all necessary information was retrieved.
- Weighted aggregate (used for overall ranking):
  - `Aggregate = 0.4·Faithfulness + 0.3·AnswerRelevancy + 0.2·ContextPrecision + 0.1·ContextRecall`.
- Judge and embeddings for evaluation were orchestrated via the benchmarking harness in `src/benchmarking/benchmark.py` (see Chapter 4 for details and caveats about judge choice and prompt settings).


### 5.1.1 Overall Model Ranking

To identify the best-performing configuration, we computed a weighted aggregate score for each (embedding × model) pair and ranked the results.

Top configurations (by weighted aggregate)
- 1) **bge‑m3 + Falcon3‑3B (Q4)**: aggregate ≈ 0.793
- 2) **bge‑m3 + Azure GPT‑5**: aggregate ≈ 0.788
- 3) **Qwen3‑0.6B (Q8) + Qwen2.5‑3B (Q4)**: aggregate ≈ 0.786

Observations
- A well-configured on‑premise stack (Falcon3‑3B with bge‑m3) achieved the highest overall RAG quality on this dataset, slightly outperforming the cloud GPT‑5 baseline when paired with the same embedding model.
- Embedding choice matters: pairs using **bge‑m3** or **Qwen3‑0.6B** consistently ranked near the top. In contrast, **e5‑large** lagged, particularly due to lower Context Precision.
- Cloud models are not uniformly dominant across all metrics; strong retrieval (good embeddings) with capable SLMs can meet or exceed cloud RAG quality on this task.

Figures
- Figure 5.1: Overall RAG Quality Ranking — `../../results/benchmarking/20250913_170609/figures/figure_B_overall_ranking.png`
- Figure 5.2: Rank Summary Table — `../../results/benchmarking/20250913_170609/figures/figure_E_rank_summary.png`


### 5.1.2 Detailed Metric Performance

This section breaks down scores for each individual metric to highlight strengths and weaknesses by configuration.

Faithfulness (factual grounding)
- Best‑of: **Qwen3‑0.6B (Q8) + Claude Opus 4.1** ≈ 0.934; **bge‑m3 + Claude Opus 4.1** ≈ 0.931; **Qwen3‑0.6B (Q8) + Azure GPT‑5** ≈ 0.922.
- Notable local result: **Falcon3‑3B (Q4)** scored ≈ 0.807 (with Qwen3‑0.6B), which is strong for an on‑prem SLM.
- Interpretation: Larger cloud models retain an edge on hallucination control, but top local pairs are competitive when retrieval is accurate.
- Figure 5.3: Faithfulness Scores — `../../results/benchmarking/20250913_170609/figures/figure_A_faithfulness.png`

Answer Relevancy (does the answer address the question?)
- Best‑of local: **bge‑m3 + Falcon3‑3B (Q4)** ≈ 0.923; **Qwen3‑0.6B (Q8) + Qwen2.5‑3B (Q4)** ≈ 0.913; **Qwen3‑0.6B (Q8) + Llama‑3.2‑3B (Q4)** ≈ 0.902.
- Cloud: **Azure GPT‑5** ≈ 0.789; **Claude Opus 4.1** ≈ 0.650; **Gemini 2.5 Pro** ≈ 0.621.
- Interpretation: Local models often produced more directly actionable answers on this dataset. Cloud models sometimes returned conservative “insufficient context” responses, which reduces this metric but is arguably safe behavior.
- Figure 5.4: Answer Relevancy Scores — `../../results/benchmarking/20250913_170609/figures/figure_A_answer_relevancy.png`

Context Precision (how relevant are the retrieved chunks?)
- Range: generally 0.53–0.59 at the top; none exceeded 0.6.
- Best‑of: **bge‑m3 + Phi‑3.5‑mini (Q4)** ≈ 0.593; **Qwen3‑0.6B (Q8) + Qwen2.5‑3B (Q4)** ≈ 0.580; **bge‑m3 + Falcon3‑3B (Q4)** ≈ 0.574.
- Underperformer: **e5‑large** pairs often around ≈ 0.43–0.46.
- Interpretation: Retrieval precision is the primary bottleneck. Embedding model selection exerts a strong influence; **bge‑m3** and **Qwen3‑0.6B** consistently help.
- Figure 5.5: Context Precision Scores — `../../results/benchmarking/20250913_170609/figures/figure_A_context_precision.png`

Context Recall (was the needed information retrieved?)
- Range: broadly **0.80–0.817** across strong pairs; best‑of ≈ 0.817 (multiple pairs).
- Interpretation: The retriever generally surfaced enough information, but not always with high precision, explaining the precision/recall gap.
- Figure 5.6: Context Recall Scores — `../../results/benchmarking/20250913_170609/figures/figure_A_context_recall.png`


### 5.1.3 Impact of Embedding Models

The embedding model substantially shapes downstream quality by determining which context is retrieved. We summarize each embedding’s profile and how it pairs with different SLMs.

Highlights
- **bge‑m3**: Most balanced profile, strong across all metrics, and frequent in top‑ranked pairs.
- **Qwen3‑0.6B (Q8)**: Lightweight but competitive; often close to bge‑m3, especially strong on Answer Relevancy and solid on Faithfulness.
- **e5‑large**: Trailing performance, particularly in Context Precision, leading to lower overall aggregates despite reasonable Faithfulness.

Figures
- Figure 5.7: Embedding Performance Profiles (radar):
  - `../../results/benchmarking/20250913_170609/figures/figure_C_profile_bge-m3.png`
  - `../../results/benchmarking/20250913_170609/figures/figure_C_profile_Qwen3-0-6B (Q8).png`
  - `../../results/benchmarking/20250913_170609/figures/figure_C_profile_e5-large.png`
- Figure 5.8: Performance Heatmaps (per metric):
  - `../../results/benchmarking/20250913_170609/figures/figure_D_heatmap_faithfulness.png`
  - `../../results/benchmarking/20250913_170609/figures/figure_D_heatmap_answer_relevancy.png`
  - `../../results/benchmarking/20250913_170609/figures/figure_D_heatmap_context_precision.png`
  - `../../results/benchmarking/20250913_170609/figures/figure_D_heatmap_context_recall.png`

Takeaways
- On this task, an on‑premise configuration with a strong embedding model (bge‑m3 or Qwen3‑0.6B) and a capable SLM (e.g., Falcon3‑3B, Qwen2.5‑3B) can match or exceed the RAG answer quality of a leading cloud model.
- Improving Context Precision—by tuning retrieval parameters, chunking, and embedding choice—offers the greatest upside. Recall is already high; raising precision should lift both relevancy and aggregate scores.
