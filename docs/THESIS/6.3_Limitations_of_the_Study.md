## 6.3 Limitations of the Study

An honest assessment of the work’s limitations is essential for contextualising the findings and ensuring academic rigour. The constraints below help explain where results may not fully generalise and where design decisions traded realism for controllability.

### 6.3.1 Single‑Node Deployment

- The benchmarks reflect a single‑node, single‑GPU topology. Observed scaling knees (e.g., c≈4–8) and tail behaviour are characteristic of this architecture rather than of a horizontally scaled cluster. A production‑grade system targeting higher concurrency would require load balancing, multi‑node inference, and queueing strategies—explicitly out of scope here.

### 6.3.2 Scope of the Evaluation (Models and Domain)

- Model coverage focused on a pragmatic set of open‑source SLMs plus a handful of cloud baselines. The RAG quality study used a single domain (UCL CS Handbook), limiting external validity. As Chapter 5.1 notes, retrieval precision was the principal bottleneck and is data‑ and embedding‑sensitive; different corpora may yield different rankings.

### 6.3.3 Effects of Quantisation

- Performance and quality results used quantised GGUF variants for on‑prem SLMs. This mirrors realistic on‑prem deployment constraints, but quantisation may depress quality and shift latency/throughput relative to full‑precision checkpoints. The study did not measure the delta between quantised and full‑precision models on identical hardware.

### 6.3.4 Estimated Cost Analysis

- Cost efficiency relies on point‑in‑time rental pricing and measured token usage distributions. As documented in Chapter 5.3, on‑prem CPR is derived from sustained RPS and an hourly node cost, while cloud CPR uses token pricing and median/p90 tokens from `results/cost/token_stats.json` via `src/throughput/token_stats_from_answers.py`. Hardware prices, energy, cooling, and operational overheads (monitoring, maintenance) are excluded; break‑even figures should be read as illustrative rather than universal.

### 6.3.5 Non‑Native Embeddings for Cloud Baselines

- For fairness, the study often paired cloud generators (e.g., Azure GPT‑5) with the same embeddings used locally (e.g., `bge‑m3`). This isolates the generator but under‑represents vendor‑optimised end‑to‑end pipelines that use provider‑native embeddings and retrieval. As observed in 5.1, embedding choice materially affects Context Precision. Using a non‑native embedding for cloud baselines improves like‑for‑like control but may understate the best achievable cloud performance.

### 6.3.6 Throughput Measured on Direct LLM Paths (Not RAG)

- Throughput (Chapter 5.2) used a short interactive prompt and capped completions against model endpoints, not the full RAG pipeline. The report explicitly cautions to re‑run with `--mode rag` for production flows (5.2 L83–L85). Consequently, measured RPS and p95 exclude retrieval, chunk formatting, and prompt assembly latencies present in `src/main.py`. Running the same experiment against `/query` would provide a truer end‑to‑end picture but was avoided due to cost (the standalone benchmark was already ~£35).

### 6.3.7 Temperature and Configuration Mismatch Across Providers

- In 5.2 the workload fixed `temperature=0.0`, but Azure enforced `1.0` via its API (5.2 L6). This configuration mismatch reduces determinism for the cloud baseline and may influence output lengths and latency, slightly biasing cross‑provider comparisons. Normalising temperatures across providers would improve comparability.

### 6.3.8 Environment Observability and Variance

- Some system metadata (e.g., GPU details) was not captured in `system-info.json` for the `vm` run (5.2 L3), limiting post‑hoc analysis. Development alternated between Mac (host‑native Ollama, no Docker GPU) and a VM (containerised Ollama). Although networking presets (`src/build_index.py` and `src/throughput/runner.py`) standardised endpoints, residual differences (unified memory on Mac, container overheads on VM) may affect micro‑benchmarks.

### 6.3.9 Evaluator and Metric Dependence

- RAG quality relies on Ragas metrics and a model judge orchestrated by `src/benchmarking/benchmark.py`. While metrics such as Faithfulness and Answer Relevancy are informative, they imperfectly proxy human judgement and can be sensitive to prompt scaffolding. The study fixed a common retrieval chain to improve fairness, but this may not be optimal for every model family, and evaluation bias cannot be ruled out without human annotation.

### 6.3.10 Residual Data Conversion Noise

- Despite the Docling‑first pipeline and post‑processing (`src/markdown_conversion/`), complex tables and edge‑case headings remain challenging. Chapter 5.1 shows that Context Precision generally stays below 0.6 across top configurations, suggesting residual structural noise and chunking heuristics still cap retrieval quality.

### 6.3.11 Query Style and Length Bias in the Testset

- The final testsets enforced formal style (`QueryStyle.PERFECT_GRAMMAR`) and fixed lengths in `src/testset/generate_testset.ipynb` to improve reliability. This increases control but deviates from real‑world, colloquial user queries, potentially inflating Answer Relevancy for models that prefer well‑formed input and penalising robustness to noisy queries.

### 6.3.12 Limited Error and Reliability Modelling

- The throughput harness retries on 429/5xx and reports errors (5.2 L14, L85), but the study did not explore sustained fault conditions, back‑pressure strategies, or long‑duration stability. Cloud providers’ queueing policies also impacted results at high concurrency (e.g., Gemini errors at c=16), complicating apples‑to‑apples comparisons.
