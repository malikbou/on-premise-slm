## 7.2 Future Work

The challenges and limitations identified in Chapter 6 suggest several clear directions to extend this work. Priorities are organised from “near‑term/high‑leverage” to “longer‑term/strategic”, with links to the relevant components and results where applicable.

### 7.2.1 System and Technical Enhancements

- Advanced RAG techniques
  - Integrate a reranker and hybrid retrieval (BM25 + dense) to lift Context Precision, which plateaued below ~0.6 across top configurations (see 5.1, Figure 5.5). Start with a lightweight cross‑encoder reranker and compare against hybrid scoring.
  - Add query rewriting and answer‑length control at the RAG API to stabilise Answer Relevancy and p95 (touchpoints: `src/main.py`, retrieval chain and prompt scaffold; 5.2 cautions to evaluate with RAG mode).
- End‑to‑end RAG throughput benchmarking
  - Re‑run capacity tests against `/query` (`src/throughput/runner.py --mode rag`) to include retrieval and prompt assembly cost. Use the same concurrency sweep and report TPS alongside RPS to handle variable output lengths. Mitigate cost by running on on‑prem SLMs first; sample cloud only where needed.
- Real‑time cost and usage telemetry
  - Add live cost panels by wiring `src/throughput/usage_probe.py` and `src/throughput/token_stats_from_answers.py` to a small dashboard, surfacing per‑model CPR and break‑even in near‑real‑time (cf. 5.3 methodology). For on‑prem, integrate power draw sampling to approximate energy‑adjusted CPR.
- Full OpenWebUI integration
  - Compare the custom FastAPI path with OpenWebUI’s built‑in RAG features. Evaluate differences in retrieval, chunking, and caching policies and their impact on precision/latency.

### 7.2.2 Continual and Governed Knowledge Base

- Incremental ingestion and re‑indexing
  - Build a pipeline to detect deltas (new/updated Markdown), re‑chunk, and update FAISS indexes without full rebuilds. Capture provenance and version tags on chunks to support rollback and auditability.
- Change‑aware retrieval
  - Incorporate recency/validity signals into retrieval (e.g., boosting newer sections or demoting deprecated ones) to handle evolving handbooks or policies.
- Governance and CI
  - Add CI checks to validate Docling conversion outputs and index integrity (non‑empty tables, heading structure), failing fast when regressions are detected (see `src/markdown_conversion/` for hooks).

### 7.2.3 Model Adaptation: Fine‑Tuning and Domain Specialisation

- Domain‑adaptive fine‑tuning
  - Explore fine‑tuning a compact SLM on the domain corpus, then combining with RAG. Hypothesis: a domain‑adapted generator plus strong embeddings may outperform a generic model with RAG alone on formal documentation.
- Embedding strategy alignment
  - Re‑evaluate with provider‑native embeddings on cloud baselines to avoid under‑representing their end‑to‑end pipelines (limitation 6.3.5). Conversely, test newer local embeddings (e.g., `bge‑m3` variants) and a reranker to target precision gains.

### 7.2.4 Broadening the Evaluation

- Expanded testset and personas
  - Grow the testset to cover more query types (procedural, comparative, tabular) and add more personas (e.g., first‑year student, international student, staff tutor). Keep Ragas conventions aligned (`keyphrases`, `keyphrases_overlap`, `keyphrases_overlap_score`; see `src/testset/generate_testset.ipynb` and the 2025‑09‑12 log entry in `docs/THESIS/PROBLEMS_SOLUTIONS.md`).
- Formal user study
  - Conduct a structured user study with UCL students to validate perceived usefulness and speed, complementing metric‑based evaluation (5.1–5.2).
- Dedicated hardware benchmarking
  - Repeat throughput/latency on purchased server hardware to remove rental VM variance and capture GPU details systematically in `system-info.json`.

### 7.2.5 Advanced Data Processing for RAG

- Robust table extraction and querying
  - Extend the Docling pipeline with targeted table reconstruction and schema normalisation for complex, multi‑page tables (see `src/markdown_conversion/pipeline.py` and post‑processors). Add table‑aware chunking and a tabular QA path to improve retrieval for policy and timetable queries.
- Section‑aware chunking
  - Investigate adaptive chunk sizes anchored to stable headings, and cross‑linking of related sections to improve precision/recall balance.

### 7.2.6 Exploring Distributed and Clustered Deployments

- Clustered SLM serving
  - Prototype horizontal scaling with multiple SLM replicas and a lightweight router. Measure RPS scaling and tail behaviour beyond the single‑node knee (5.2). Explore batching for micro‑models where it is effective without harming latency SLOs.
- Queueing and priority control
  - Introduce admission control and priority queues to keep p95 bounded under bursts; compare with cloud providers’ queueing behaviour.

### 7.2.7 Domain‑Specific RAG Implementation

- Apply the pipeline to proprietary domains (finance, legal, support) where document structure and table density differ significantly. Measure business‑relevant metrics (first‑pass answer rate, correction rate) rather than only Ragas aggregates.

### 7.2.8 Operational Hardening

- Observability and guardrails
  - Add structured tracing for retrieval and generation steps; log chunk IDs and scores to debug false positives/negatives. Introduce response filters (PII, toxicity) where appropriate.
- Reproducibility and release process
  - Pin eval and serving configurations (temperatures, max tokens) per provider to avoid drift (cf. 5.2 temperature mismatch). Automate figure and table generation directly from `results/` to keep the thesis artefacts reproducible.

In short, the near‑term emphasis should be on improving retrieval precision (reranking, hybrid search, table‑aware processing) and measuring true end‑to‑end performance on `/query`, while the strategic track explores continual ingestion, domain adaptation, and horizontal scaling. Together these would move the system from a strong prototype to a robust, governable, and scalable on‑prem alternative.
