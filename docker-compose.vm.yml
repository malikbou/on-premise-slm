# This file is an add-on for the main docker-compose.yml.
# It should ONLY be used in a production environment with an NVIDIA GPU.
#
# How to use:
# docker-compose -f docker-compose.yml -f docker-compose.vm.yml up -d



services:
  ollama:
    image: ollama/ollama
    gpus: all
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    container_name: ollama
    restart: unless-stopped
    environment:
      - NVIDIA_VISIBLE_DEVICES=all

  # Override RAG API services to use containerized Ollama on the VM
  rag-api-bge:
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434

  rag-api-qwen3:
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434

  rag-api-e5:
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434

  # Ensure Open WebUI points to containerized Ollama on VM
  open-webui:
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434

  # Do-not-autostart benchmarking; enable via profile
  benchmarker:
    profiles: ["benchmark"]
    command: ["python", "-u", "src/benchmarking/benchmark.py", "--preset", "vm"]
    depends_on:
      - litellm
      - rag-api-bge
      - rag-api-qwen3
      - rag-api-e5

  # Run throughput runner inside the Docker network; start via profile only
  throughput-runner:
    build: .
    command: ["python", "src/throughput/runner.py", "--mode", "rag", "--platform-preset", "vm", "--rag-base", "http://rag-api-bge:8000", "--rag-testset", "/app/data/testset/ucl-cs_single_hop_testset_gpt-4.1_20250906_111904.json", "--repetitions", "1", "--requests", "20", "--concurrency", "1,2,4,8,16", "--skip-cloud"]
    depends_on:
      - litellm
      - rag-api-bge
      - rag-api-qwen3
      - rag-api-e5
    volumes:
      - ./src:/app/src
      - ./data:/app/data:ro
      - ./results:/app/results
    env_file:
      - .env
    profiles: ["throughput"]

volumes:
  ollama_data:
