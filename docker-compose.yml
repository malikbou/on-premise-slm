services:
  litellm:
    image: ghcr.io/berriai/litellm:main-stable
    ports:
      - "4000:4000"
    volumes:
      - ./config.yaml:/app/config.yaml:ro
    command: ["--config", "/app/config.yaml", "--port", "4000"]
    env_file:
      - .env
    container_name: litellm

  rag-api:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./src:/app/src
      - rag_cache:/app/.rag_cache:ro
    depends_on:
      - litellm
    container_name: rag-api
    env_file:
      - .env
    environment:
      - INDEX_DIR=/app/.rag_cache/faiss_index

  index-builder:
    build: .
    command: ["python", "src/build_index.py"]
    volumes:
      - ./data:/app/data:ro
      - rag_cache:/app/.rag_cache
    env_file:
      - .env
    environment:
      - HANDBOOK_MD_PATH=/app/data/cs-handbook.md
    extra_hosts:
      - "host.docker.internal:host-gateway"

  multi-index-builder:
    build: .
    command: ["python", "src/build_index.py"]
    volumes:
      - ./data:/app/data:ro
      - rag_cache:/app/.rag_cache
    env_file:
      - .env
    environment:
      - HANDBOOK_MD_PATH=/app/data/cs-handbook.md
      - EMBEDDING_MODELS=bge-m3,hf.co/Qwen/Qwen3-Embedding-0.6B-GGUF:Q8_0,yxchia/multilingual-e5-large-instruct
    extra_hosts:
      - "host.docker.internal:host-gateway"

  benchmarker:
    build: .
    command: ["python", "src/benchmarking/benchmark.py"]
    volumes:
      - ./data:/app/data:ro
      - ./testset:/app/testset:ro
      - ./results:/app/results
    depends_on:
      - rag-api-bge
      - rag-api-qwen3
      - rag-api-e5
    environment:
      - EMBEDDING_API_MAP=bge-m3=http://rag-api-bge:8000,hf.co/Qwen/Qwen3-Embedding-0.6B-GGUF:Q8_0=http://rag-api-qwen3:8000,yxchia/multilingual-e5-large-instruct=http://rag-api-e5:8000
    env_file:
      - .env

  rag-api-bge:
    build: .
    ports:
      - "8001:8000"
    volumes:
      - ./src:/app/src
      - rag_cache:/app/.rag_cache:ro
    depends_on:
      - litellm
    container_name: rag-api-bge
    env_file:
      - .env
    environment:
      - INDEX_DIR=/app/.rag_cache/bge_m3/faiss_index
      - EMBEDDING_MODEL=bge-m3
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:8000/info"]
      interval: 10s
      timeout: 5s
      retries: 5
    extra_hosts:
      - "host.docker.internal:host-gateway"

  rag-api-qwen3:
    build: .
    ports:
      - "8002:8000"
    volumes:
      - ./src:/app/src
      - rag_cache:/app/.rag_cache:ro
    depends_on:
      - litellm
    container_name: rag-api-qwen3
    env_file:
      - .env
    environment:
      - INDEX_DIR=/app/.rag_cache/qwen3/faiss_index
      - EMBEDDING_MODEL=hf.co/Qwen/Qwen3-Embedding-0.6B-GGUF:Q8_0
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:8000/info"]
      interval: 10s
      timeout: 5s
      retries: 5
    extra_hosts:
      - "host.docker.internal:host-gateway"

  rag-api-e5:
    build: .
    ports:
      - "8003:8000"
    volumes:
      - ./src:/app/src
      - rag_cache:/app/.rag_cache:ro
    depends_on:
      - litellm
    container_name: rag-api-e5
    env_file:
      - .env
    environment:
      - INDEX_DIR=/app/.rag_cache/yxchia_multilingual_e5_large_instruct/faiss_index
      - EMBEDDING_MODEL=yxchia/multilingual-e5-large-instruct
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:8000/info"]
      interval: 10s
      timeout: 5s
      retries: 5
    extra_hosts:
      - "host.docker.internal:host-gateway"

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    ports:
      - "3000:8080"
    volumes:
      - open_webui_data:/app/backend/data
    environment:
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - WEBUI_SECRET_KEY=your-secret-key-here
      - WEBUI_NAME=RAG Evaluation WebUI
    extra_hosts:
      - "host.docker.internal:host-gateway"
    container_name: open-webui
    restart: unless-stopped

  ollama-benchmark:
    build: .
    command: ["python", "src/ollama_benchmark.py", "--questions", "5", "--concurrency", "2"]
    volumes:
      - ./src:/app/src
      - ./testset:/app/testset:ro
      - ./results:/app/results
    extra_hosts:
      - "host.docker.internal:host-gateway"
    container_name: ollama-benchmark
    env_file:
      - .env

  simple-benchmarker:
    build: .
    command: ["python", "src/benchmark_simple.py", "--preset", "vm", "--mode", "all"]
    volumes:
      - ./data:/app/data:ro
      - ./testset:/app/testset:ro
      - ./results:/app/results
    depends_on:
      - litellm
      - rag-api-bge
      - rag-api-qwen3
      - rag-api-e5
    container_name: simple-benchmarker
    env_file:
      - .env

volumes:
  rag_cache:
  open_webui_data:
