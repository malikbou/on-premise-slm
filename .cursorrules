# RAG Evaluation Thesis Project - Cursor Rules

## Project Context
This is an academic thesis project focused on benchmarking RAG (Retrieval-Augmented Generation) applications for deployment on GPU VMs. The goal is to evaluate document processing, testset generation, RAG performance, and throughput metrics.

## Technology Stack
- **Backend**: FastAPI, Python 3.11
- **Vector Store**: FAISS with multiple embedding models
- **LLM Integration**: LangChain, Ollama, LiteLLM proxy
- **Evaluation**: RAGAS framework (faithfulness, answer_relevancy, context_precision, context_recall)
- **Load Testing**: httpx, asyncio for concurrency testing
- **Containerization**: Docker, Docker Compose
- **Document Processing**: Unstructured, planned Docling integration
- **Data Visualization**: matplotlib, pandas

## Key Components
1. **RAG API** (`src/main.py`) - FastAPI service with retrieval and generation
2. **RAG Benchmarking** (`src/benchmark.py`) - RAGAS evaluation with parallel metrics for answer quality
3. **Index Builder** (`src/build_index.py`) - FAISS vector store creation
4. **Throughput Testing** (`load-testing/openai_llm_benchmark.py`) - Primary throughput/latency testing (vLLM vs Ollama)
5. **Document Processing** (planned) - Docling-based PDF to Markdown conversion
6. **Testset Generation** (planned) - 100 CS handbook questions for evaluation

## Deployment Environment
- **Target**: Vast.ai GPU VM with 12GB VRAM
- **GPU Memory Management**: Critical for model lifecycle during benchmarking
- **Models**: Must fit in 12GB VRAM individually
- **Concurrency**: Memory-aware batch processing required

## Coding Guidelines

### Python Standards
- Use type hints throughout
- Follow async/await patterns for I/O operations
- Implement proper error handling with retries for LLM API calls
- Use environment variables for configuration
- Follow PEP 8 with Black formatting

### RAG-Specific Patterns
- Always implement `keep_alive=0` for Ollama models to manage memory
- Use batch processing for large testsets to optimize memory usage
- Implement parallel metric evaluation with ThreadPoolExecutor
- Handle embedding model switching gracefully
- Cache vector stores and reuse when possible

### Performance Considerations
- Memory optimization is critical for large document sets
- Implement streaming/chunked processing for testset generation
- Use semaphores for concurrency control in benchmarks
- Profile memory usage during evaluation runs
- Implement proper model lifecycle management

### Error Handling
- Implement exponential backoff for API retries
- Handle JSON parsing failures in LLM responses
- Graceful degradation when models fail to load
- Comprehensive logging for debugging evaluation runs

### Testing & Evaluation
- Always save raw responses for audit trails
- Generate timestamped result directories
- Include metadata in benchmark results (model versions, parameters)
- Implement summary reporting for easy comparison
- Save evaluation datasets for reproducibility

## Constraints & Requirements
- **GPU Memory**: Be mindful of VRAM usage with large models
- **Academic Standards**: All experiments must be reproducible
- **Docker Environment**: All services must run in containers
- **Multi-Model Support**: Support both local (Ollama) and cloud LLMs
- **Evaluation Rigor**: Use established metrics (RAGAS) for credibility

## File Organization
- Configuration in `config.yaml` and `.env`
- Source code in `src/` directory
- Results timestamped in `results/` with clear naming
- Test data in `testset/` directory
- Documentation in project root
- Docker configs at project root level

## Do NOT
- Hardcode API keys or sensitive values
- Create files outside the established structure
- Modify core evaluation metrics without documentation
- Skip error handling in async operations
- Ignore memory management for large datasets
- Use non-reproducible random seeds in evaluation

## Agent-Specific Instructions
Task-specific agent instructions are available in `.agent-prompts/` directory:
- `document-processor.md`: Docling-based PDF to Markdown conversion
- `testset-generator.md`: CS handbook question generation (100 questions)
- `rag-benchmarker.md`: RAGAS evaluation with GPU memory management
- `throughput-tester.md`: Load testing enhancements with hardware info
- `MULTI_AGENT_COMPETITION.md`: Framework for running multiple agents on same task

**Usage**: When working on a specific task, read the corresponding instruction file to understand the context, requirements, and success criteria.

## Documentation Automation Rules

### Automatic README Updates
**CRITICAL**: Always update README.md when making significant changes to:
- Adding new components or services
- Modifying configuration options
- Changing deployment procedures
- Adding new agent instructions
- Updating cross-platform requirements

### Thesis Documentation Automation
**User Manual Updates**: When making user-facing changes, automatically update:
- Installation procedures in README.md
- Configuration examples
- Usage workflows (Mac local â†’ Vast.ai deployment)
- Troubleshooting sections

**System Manual Updates**: When making technical changes, automatically update:
- Architecture documentation
- API documentation
- Configuration reference
- Technical implementation details

### Documentation Trigger Patterns
**Auto-update documentation when**:
- New files added to `src/`, `.agent-prompts/`, or configuration
- Docker services added/modified in docker-compose files
- Environment variables added/changed
- New benchmarking or testing capabilities added
- Cross-platform compatibility changes

### Documentation Standards
- **Academic Quality**: All documentation suitable for thesis appendices
- **Reproducibility**: Complete setup instructions for any platform
- **Professional Presentation**: Clear structure, proper formatting
- **Version Control**: Track documentation changes with implementation changes

## Mandatory Agent Planning Process

### CRITICAL: Plan Before Execute
**ALL AGENTS MUST:**
1. **Create detailed implementation plan** before writing any code
2. **Present plan to user for approval** - wait for explicit confirmation
3. **Only proceed after user approval** - no code execution without consent
4. **Break complex tasks into phases** with clear milestones
5. **Identify potential risks and mitigation strategies**
6. **Estimate time and resources required**

**Planning Template Required:**
```
## Implementation Plan: [Task Name]

### Phase 1: [Description]
- Objectives: [list]
- Deliverables: [list]
- Risks: [list]
- Success Criteria: [list]

### Phase 2: [Description]
[same structure]

### Resource Requirements:
- Time estimate: [X hours/days]
- Dependencies: [list]
- Hardware/software needs: [list]

### Approval Request:
"Please approve this plan before I proceed with implementation."
```

## Thesis Documentation Automation

### Academic Writing Standards
**Automatically maintain thesis-quality documentation:**

**User Manual Auto-Updates (docs/USER_MANUAL.md):**
- Installation procedures (Mac local + Vast.ai VM)
- Configuration examples with cross-platform notes
- Step-by-step usage workflows
- Troubleshooting with platform-specific solutions
- Hardware requirements and compatibility notes

**System Manual Auto-Updates (docs/SYSTEM_MANUAL.md):**
- Architecture diagrams with component interactions
- API documentation with request/response examples
- Configuration reference with environment variables
- Technical implementation details and design decisions
- Cross-platform deployment considerations

**Trigger Conditions for Documentation Updates:**
- New services added to docker-compose files
- Environment variables added/modified
- New API endpoints or configuration options
- Cross-platform compatibility changes
- Performance benchmarking results
- Agent instructions updated

### Reference Management Requirements
**Academic Citation Standards:**
- Use IEEE citation style consistently
- Reference all external libraries, frameworks, and tools
- Cite all research papers, documentation, and online resources
- Include proper figure captions with sources
- Maintain comprehensive reference list in bibliography format

**Auto-Reference Triggers:**
- When using new libraries or frameworks
- When implementing algorithms from papers
- When including external data or benchmarks
- When referencing performance comparisons

### Report Structure Automation
**Maintain UCL Thesis Chapter Structure:**

**Chapter 1: Introduction**
- Clear problem statement and project goals
- Scope definition and success criteria
- Overview of methodology and approach

**Chapter 2: Context/Background**
- Literature review of RAG evaluation methods
- Survey of embedding models and LLMs
- Related work in throughput testing and benchmarking

**Chapter 3: Requirements and Analysis**
- System requirements (functional and non-functional)
- Cross-platform compatibility requirements
- Performance and scalability requirements

**Chapter 4: Design and Implementation**
- Architecture overview with component diagrams
- Implementation details of key components
- Cross-platform configuration management

**Chapter 5: Testing and Evaluation**
- RAG evaluation methodology using RAGAS
- Throughput testing strategy and results
- Cross-platform performance comparison

**Chapter 6: Conclusions**
- Summary of achievements vs. goals
- Critical evaluation of results
- Future work and recommendations

### Professional Code Standards
**Natural, Human-Like Code Quality:**
- Write clean, readable code that doesn't look AI-generated
- Use descriptive variable names and meaningful comments
- Follow established conventions and patterns
- Implement proper error handling and logging
- Structure code logically with appropriate abstraction
- Avoid overly verbose or mechanical coding patterns
- Write documentation that sounds natural and professional

**Code Quality Indicators:**
- Functions have clear single responsibilities
- Variable names reflect business domain concepts
- Comments explain "why" not just "what"
- Error messages are helpful and user-friendly
- Code structure reflects natural problem decomposition

## Thesis-Specific Notes
- Document all hyperparameters and model versions
- Maintain clear audit trail of experiments
- Focus on comparing embedding models and LLM performance
- Optimize for deployment on GPU infrastructure
- Prepare for academic presentation of results
- **Memory Management**: Critical for 12GB VRAM constraints on Vast.ai
